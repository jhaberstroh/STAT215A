\batchmode
\makeatletter
\def\input@path{{/Users/Adam/Dropbox/Stat215A-2013/Labs/Lab2/Lab//}}
\makeatother
\documentclass[english]{article}\usepackage{graphicx, color}
%% maxwidth is the original width if it is less than linewidth
%% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\definecolor{fgcolor}{rgb}{0.2, 0.2, 0.2}
\newcommand{\hlnumber}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlfunctioncall}[1]{\textcolor[rgb]{0.501960784313725,0,0.329411764705882}{\textbf{#1}}}%
\newcommand{\hlstring}[1]{\textcolor[rgb]{0.6,0.6,1}{#1}}%
\newcommand{\hlkeyword}[1]{\textcolor[rgb]{0,0,0}{\textbf{#1}}}%
\newcommand{\hlargument}[1]{\textcolor[rgb]{0.690196078431373,0.250980392156863,0.0196078431372549}{#1}}%
\newcommand{\hlcomment}[1]{\textcolor[rgb]{0.180392156862745,0.6,0.341176470588235}{#1}}%
\newcommand{\hlroxygencomment}[1]{\textcolor[rgb]{0.43921568627451,0.47843137254902,0.701960784313725}{#1}}%
\newcommand{\hlformalargs}[1]{\textcolor[rgb]{0.690196078431373,0.250980392156863,0.0196078431372549}{#1}}%
\newcommand{\hleqformalargs}[1]{\textcolor[rgb]{0.690196078431373,0.250980392156863,0.0196078431372549}{#1}}%
\newcommand{\hlassignement}[1]{\textcolor[rgb]{0,0,0}{\textbf{#1}}}%
\newcommand{\hlpackage}[1]{\textcolor[rgb]{0.588235294117647,0.709803921568627,0.145098039215686}{#1}}%
\newcommand{\hlslot}[1]{\textit{#1}}%
\newcommand{\hlsymbol}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlprompt}[1]{\textcolor[rgb]{0.2,0.2,0.2}{#1}}%

\usepackage{framed}
% \makeatletter
% \newenvironment{kframe}{%
%  \def\at@end@of@kframe{}%
%  \ifinner\ifhmode%
%   \def\at@end@of@kframe{\end{minipage}}%
%   \begin{minipage}{\columnwidth}%
%  \fi\fi%
%  \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
%  \colorbox{shadecolor}{##1}\hskip-\fboxsep
%      % There is no \\@totalrightmargin, so:
%      \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
%  \MakeFramed {\advance\hsize-\width
%    \@totalleftmargin\z@ \linewidth\hsize
%    \@setminipage}}%
%  {\par\unskip\endMakeFramed%
%  \at@end@of@kframe}
% \makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
% \newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}
\usepackage[T1]{fontenc}
\usepackage[latin9]{inputenc}
\usepackage{geometry}
\geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
\usepackage{fancyhdr}
\pagestyle{fancy}
\setlength{\parskip}{\smallskipamount}
\setlength{\parindent}{0pt}
\usepackage{amsthm}
\usepackage{amsmath}

\makeatletter

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% LyX specific LaTeX commands.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Textclass specific LaTeX commands.
\numberwithin{equation}{section}
\numberwithin{figure}{section}

\@ifundefined{date}{}{\date{}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.
\pagestyle{empty} 

\makeatother

\usepackage{babel}
\begin{document}

\title{Lab 2 - Linguistic Survey\\
Stat 215A, Fall 2013}


\author{Your name}

\maketitle

\section{Introduction}

In the study of dialects, there is a long-standing connundrum about the relation of regional dialects and spatial distribution of particular linguistic choices. While it is evidently possible to qualitatively identify a person's regional dialect in practice, the analysis of any particular linguistic choice does not generate these boundaries. Here, we attempt to take the "blind" approach: to use extensive data across multiple linguistic features to generate those boundaries that are spatially pertinent, without explicit reference to the boundaries themselves.

\section{The Data}

This data is linguistic survey data collected from the internet pertaining to a person's common english language word choice. 

\subsection{Data quality and cleaning}

For simplicity, I deleted all rows with instances of NA from my dataset. This removed about 2\% of the data, which I deemed acceptable. Many question responses were missing, labelled with a zero. These responses were left in the data set and handled further downstream.

To create a spatial grid of data that did not depend on the local density of responses, I used a nearest-neighbor search. For each point on the grid, the k=100 nearest neighbors were found. Then, these 100 neighbors' "binarized" responses were added together. At this point, zeros were omitted from responses; thus, each question was not necessarily normalized to 100 responses. 

It was assumed that zero-responses were from either respondent uncertainty or simple malfunction. Assuming the malfunctions to be randomly distributed and the respondent uncertainty to be meaningful, the resulting bins of fraction of non-zero responses of the 100 nearest neighbors were assumed to be meaningful. 

\subsection{Exploratory Data Analysis}

<<init_ling, cache=FALSE, warning=FALSE, message=FALSE, echo=FALSE >>=
source("jhaberstroh_ling_methods.R")

# ----------------------------------------# Perform cleaning 
ling.data <- filter(ling.data, !is.na(lat) & !is.na(long))
# ----------------------------------------# Create binary dataset 
ling.identify <- c('ID','CITY','STATE','ZIP','lat','long')
ling.subset <- c('CITY','STATE','ZIP','lat','long')
ling.ans <- names(ling.data)[!(names(ling.data) %in% ling.identify)]
ling.data.bin <- BinarizeAnswers(ling.data[, !names(ling.data) %in% ling.subset],'ID')
ling.data.bin <- inner_join(ling.data[,ling.identify], ling.data.bin, by='ID')
remove(ling.identify)
remove(ling.subset)
remove(ling.ans)
# ----------------------------------------# Create training/validation dataset 
n.train <- 10000
n.rows <- nrow(ling.data.bin)
ling.samples.train <- sample(n.rows,n.train)
ling.data.bin.train <- ling.data.bin[ling.samples.train,]
ling.data.bin.valid <- ling.data.bin[!seq(1, n.rows) %in% ling.samples.train,]
@

<<hclust_people, echo=FALSE, warning=FALSE, message=FALSE, fig.cap="Heirarchical clustering on individuals shows excessive sensitivity to particular outliers. Data may give a sense of the natural variation of responses.", fig.height=4>>=
# ----------------------------------------
# Perform heirarchical clustering on all of the data 
# ----------------------------------------
SeguyDist <- function(data) dist(data, method=function(x, y)  sum(x!=y))

ling.ans <- names(ling.data)[!names(ling.data) %in% c('ID', 'CITY','STATE','ZIP','lat','long')]
n.sample <- 2000
ling.data.mainland <- filter(ling.data, -65 > long & long > -125 & 50 > lat & lat > 25)
ling.data.hclust <- ling.data.mainland[sample(nrow(ling.data.mainland),n.sample), ]
ling.data.hclust <- ApplyHClust(ling.data.hclust, 
                                dist_method=SeguyDist, 
                                cluster.cols=ling.ans)
ggplot(ling.data.hclust, aes(x=long, y=lat, color=cluster8)) +
  geom_point(alpha=.8) +
  scale_color_brewer(palette = 'Set1')
@

To explore the data, first I took all of the responses from the data, and I generated a distance function between two elements. This distance function is that used by Seguy: the distance is the number of elements which are not shared in common. This allows us to immediately begin clustering data with a non-metric method. Thus, I began by using a heirarchical clustering.

It was clear from this initial clustering that this did not create a geographical distribution of responses. Furthermore, the maximum variation generated by individiuals was larger than that generated by spatial variation. This indicated that spatial aggregation would be necessary to reduce variance from individuals within a region, to be able to see the broader trend.

I also wanted to explore the number of redundant area codes that showed up in the raw data. 


<<ling_areacode, echo=FALSE, fig.cap="Redunancy in area code shows a strong signal near Minneapolis. This feature emerges in the heirarchical clustering of spatially coarse grained data.", fig.height=4>>=
ling.data.bin.mainland <- filter(ling.data.bin, lat < 53 & long > -140)
ling.data.bin.mainland.latlongagg <- group_by(ling.data.bin.mainland, lat, long) %>% 
  summarize(n=n())
max = head(ling.data.bin.mainland.latlongagg[
    with(ling.data.bin.mainland.latlongagg, order(-n)),],100)
ggplot(data=NULL) +
  plot.map + 
  plot.blank +
  geom_point(data=max, aes(x=long, y=lat, color=n), size=3, alpha=.9)
@


\section{Dimension reduction methods}

<<spatial_hclust, echo=FALSE, message=FALSE, warning=FALSE, fig.cap="Clusters formed from spatial data shows much more regularity and the iterative emergece of cultural regions is apparent -- save for the outliers in Minneapolis and Denver.", fig.height=4>>=
# # Create nearest neighbor spatial gridding
# # ----------------------------------------
# ling.identify <- c('ID','CITY','STATE','ZIP','lat','long')
# ling.ans <- names(ling.data.bin)[!names(ling.data.bin) %in% ling.identify]
# spatial.nn.bin <- GetNNSum(ling.data.bin, 
#                            CreateGrid(seq(-125,-65,.5), seq(25,50,.5)), 
#                            cols.pos=c('lat','long'), cols.sum=ling.ans)
# # Perform hclust on nn grid
# # ----------------------------------------
# spatial.nn.bin.copy <- ApplyHClust(spatial.nn.bin, dist_method=dist,
#                                    cluster.cols=ling.ans)
# 
# save(spatial.nn.bin, spatial.nn.bin.copy, file='hclust_file.Rda')

# Plot the nn spatial map for cluster
# ----------------------------------------
load('hclust_file.Rda')
p3 <- ggplot(spatial.nn.bin.copy) +
  geom_point(aes(x=long,y=lat,color=cluster3), shape=15, size = 4, alpha=.6) +
  plot.map +
  plot.blank +
  scale_color_brewer(palette = 'Set3')
p5 <- ggplot(spatial.nn.bin.copy) +
  geom_point(aes(x=long,y=lat,color=cluster5), shape=15, size = 4, alpha=.6) +
  plot.map +
  plot.blank +
  scale_color_brewer(palette = 'Set3')
p6 <- ggplot(spatial.nn.bin.copy) +
  geom_point(aes(x=long,y=lat,color=cluster6), shape=15, size = 4, alpha=.6) +
  plot.map +
  plot.blank +
  scale_color_brewer(palette = 'Set3')
p8 <- ggplot(spatial.nn.bin.copy) +
  geom_point(aes(x=long,y=lat,color=cluster8), shape=15, size = 4, alpha=.6) +
  plot.map +
  plot.blank +
  scale_color_brewer(palette = 'Set3')
multiplot(p3,p5,p6,p8, cols=2)
@

\subsection{Hierarchical Clustering of spatialized data}

To produce more stable clusters, the sptailization method described in the data cleaning was used. After performing this clustering, a standard minkowski distance was used for the clustering. This is a reasonable metric to use because the data now spans the range of 0-100, and a minkowski distance will create more distance between quite similar pairs and less distance between quite dissimilar pairs. This will reduce the effect of extreme outliers.

When we analyze the clusters generated, we find that the heirarchical clustering captures layers of relatedness that first separates Northeast, South, and West, then it goes on to separate Florida from the south, Pennsylvania from the Northeast, and to create a band between Wisconsin/Minnesota/Wisconsin and the South or East. To my cultural understanding, all of these distinctions are relevant and interesting. 

One of the problems with this clustering, though, is the association of spatially distinct clusters with one another. Further cutting of the tree, as shown in the Cluster-8 diagram, does not serve to resolve these associations; it instead creates groups near the borders of previous regions that likely have intermediate linguistic properties.

\subsection{PCA binning}

After analyzing this data, it is interesting to ask whether there are questions with maximal and minimal variance that vary from region to region. The natural analysis for this is a principal component analysis -- to see whether there are principal components within subsets of our macroscopic dataset.

<<cluster_PCA, echo=FALSE, warning=FALSE,message=FALSE, fig.cap='PCA analysis for the south, i.e. cluster 3 of 6 from the cluster6 analysis', fig.height=3>>=
ling.identify <- c('ID','CITY','STATE','ZIP','lat','long')
ling.ans <- names(spatial.nn.bin)[!(names(spatial.nn.bin) %in% ling.identify)]
nn.pca.c6.3 <- CenteredPCA(filter(spatial.nn.bin.copy, cluster6 == 3),ling.ans)
p1 <- qplot(seq_along(nn.pca.c6.3$sdev), nn.pca.c6.3$sdev) +
  scale_y_log10()
p1
@

<<cluster_PCA_2, echo=FALSE, warning=FALSE,message=FALSE, fig.cap='PCA analysis for the south, i.e. cluster 3 of 6 from the cluster6 analysis', fig.height=4>>=
p2 <- PCAComponentPlot(nn.pca.c6.3$rot, 20,end=TRUE, title='Loading of lowest eigenvectors for the south')
p2
@

<<cluster_PCA_3, echo=FALSE, warning=FALSE,message=FALSE, fig.cap='PCA analysis for the south, i.e. cluster 3 of 6 from the cluster6 analysis',fig.height=4>>=
PCAComponentPlot(nn.pca.c6.3$rot, 20,end=FALSE, title='Loading of largest eigenvectors for the south')
@

In this analysis, it seems that the lowest eigenvectors live in some sort of textured subspace of the data, while the largest eigenvectors are pretty well distributed among all of the responses. There are a few questions that stick out above the signal for the largest eigenvectors, but their loadings are pretty far from 1.

\section{Stability of findings to perturbation}
There was not sufficient time for thorough stability analysis, but the results were stable to a coarsening of the grid by an area factor of 4, as demonstrated below. Many of the fine features of the borders disappear, but the important regional trends remain.


<<coarse_cluster, echo=FALSE, fig.cap='A coarser validation of the clustering analysis', fig.height=4>>=
# Create nearest neighbor spatial gridding
# ----------------------------------------
ling.identify <- c('ID','CITY','STATE','ZIP','lat','long')
ling.ans <- names(ling.data.bin)[!names(ling.data.bin) %in% ling.identify]
spatial.nn.bin <- GetNNSum(ling.data.bin, 
                           CreateGrid(seq(-125,-65), seq(25,50)), 
                           cols.pos=c('lat','long'), cols.sum=ling.ans)
# Perform hclust on nn grid
# ----------------------------------------
spatial.nn.bin.copy <- ApplyHClust(spatial.nn.bin, dist_method=dist,
                                   cluster.cols=ling.ans)

p3 <- ggplot(spatial.nn.bin.copy) +
  geom_point(aes(x=long,y=lat,color=cluster3), shape=15, size = 4, alpha=.6) +
  plot.map +
  plot.blank +
  scale_color_brewer(palette = 'Set3')
p5 <- ggplot(spatial.nn.bin.copy) +
  geom_point(aes(x=long,y=lat,color=cluster5), shape=15, size = 4, alpha=.6) +
  plot.map +
  plot.blank +
  scale_color_brewer(palette = 'Set3')
p6 <- ggplot(spatial.nn.bin.copy) +
  geom_point(aes(x=long,y=lat,color=cluster6), shape=15, size = 4, alpha=.6) +
  plot.map +
  plot.blank +
  scale_color_brewer(palette = 'Set3')
p8 <- ggplot(spatial.nn.bin.copy) +
  geom_point(aes(x=long,y=lat,color=cluster8), shape=15, size = 4, alpha=.6) +
  plot.map +
  plot.blank +
  scale_color_brewer(palette = 'Set3')
multiplot(p3,p5,p6,p8, cols=2)
@

\section{Conclusion}
From this blind analysis, we were able to extract a culturally relevant partition of the united states. This analysis did not leverage much domain knowledge of linguistics; a deeper analysis that included correlations and clustering of questions themselves into categories would be able to more robustly analyze regions without resulting in a "crumbly" edge as the number of clusters increased.

Furthermore, a predictive aspect to the analysis seems possible from this starting point. Given the large individual variability discovered from the first clustering, there would be much to learn from a predictive methodology. Ways of accounting for the variability that is spread across all of the answers from within region to region will be crucial for prediction.

\end{document}
